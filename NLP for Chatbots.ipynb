{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS TAGGING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T08:13:12.742375Z",
     "start_time": "2019-09-24T08:13:12.563797Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T08:13:15.959811Z",
     "start_time": "2019-09-24T08:13:15.924999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am VERB\n",
      "learning VERB\n",
      "how ADV\n",
      "to PART\n",
      "build VERB\n",
      "chatbots NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I am learning how to build chatbots')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T08:20:00.806752Z",
     "start_time": "2019-09-24T08:20:00.627235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Google 96 NNP compound Xxxxx True False\n",
      "release release 92 NN ROOT xxxx True False\n",
      "\" \" 97 `` punct \" False False\n",
      "Move Move 96 NNP nmod Xxxx True True\n",
      "Mirror Mirror 96 NNP nmod Xxxxx True False\n",
      "\" \" 97 '' punct \" False False\n",
      "AI AI 96 NNP compound XX True False\n",
      "experiment experiment 92 NN appos xxxx True False\n",
      "that that 90 WDT nsubj xxxx True True\n",
      "matches match 100 VBZ relcl xxxx True False\n",
      "your -PRON- 90 PRP$ poss xxxx True True\n",
      "pose pose 92 NN dobj xxxx True False\n",
      "from from 85 IN prep xxxx True True\n",
      "80,100 80,100 93 CD nummod dd,ddd False False\n",
      "images image 92 NNS pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u'Google release \"Move Mirror\" AI experiment that matches your pose from 80,100 images')\n",
    "for token in doc1:\n",
    "    print(token.text, token.lemma_, token.pos, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T08:21:03.388736Z",
     "start_time": "2019-09-24T08:21:03.304996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- 95 PRP nsubj X True True\n",
      "am be 100 VBP aux xx True True\n",
      "learning learn 100 VBG ROOT xxxx True False\n",
      "how how 86 WRB advmod xxx True True\n",
      "to to 94 TO aux xx True True\n",
      "build build 100 VB xcomp xxxx True False\n",
      "chatbots chatbot 92 NNS dobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u'I am learning how to build chatbots')\n",
    "for token in doc2:\n",
    "        print(token.text, token.lemma_, token.pos, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| TEXT | ACTUAL TEXT OR WORD BEING PROCESSED |\n",
    "| ---- | ----------------------------------- |\n",
    "| LEMMA | Root form of word being processed |\n",
    "| POS | Part of Speech of the word |\n",
    "| TAG | They express POS |\n",
    "| DEP | Syntactic dependency |\n",
    "| SHAPE | Shape of the Word |\n",
    "| ALPHA | Is the token an alpha character? |\n",
    "| STOP | Is the word a stop word or part of stop list? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming** : Does the job in a crude way, heuristic way that chops off the the ends of words, assuming that the remaining word is what we are actually looking for, but it includes the removal of derivational affixes.<br><br>\n",
    "**Lemmatization**: Tries to do the job more elegentaly with the use of a vocabulary and morphological analysis of words. It tries its best to remove infectional endings only and return the dictionary form of a word, known as the lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T08:56:00.471520Z",
     "start_time": "2019-09-24T08:56:00.465536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chuckle']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "lemmatizer('chuckles', 'NOUN')                                 # 2nd param is token's part-of-speech tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T08:56:18.548277Z",
     "start_time": "2019-09-24T08:56:18.541296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blaze']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('blazing', 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T08:56:31.433439Z",
     "start_time": "2019-09-24T08:56:31.428452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fast']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('fastest', 'ADJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To understand difference between stemmer and lemmatizer, use NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T08:59:14.935742Z",
     "start_time": "2019-09-24T08:59:14.928760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastest\n",
      "fastest\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "print(porter_stemmer.stem('fastest'))\n",
    "print(snowball_stemmer.stem('fastest'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "- Process of finding and classifying named entities existing in the given text into pre-defined categories.\n",
    "- Hugely dependent on the knowledge base used to train NE extraction algorithm.\n",
    "- spaCy comes with a very fast entity recognition model that is capable of identifying entity phrases from a given document.\n",
    "- Entities can be of different types, such as person, location, organiation, dates, numerals, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:05:15.355643Z",
     "start_time": "2019-09-24T09:05:15.322795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Mountain View GPE\n",
      "California GPE\n",
      "109.65 billion uS dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "my_string = u\"Google has its headquarters in Mountain View, California having revenue iamounted to 109.65 billion uS dollars\"\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:14:17.838482Z",
     "start_time": "2019-09-24T09:14:17.803611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Zuckerberg PERSON\n",
      "May 14,1984 DATE\n",
      "New York GPE\n",
      "American NORP\n",
      "Facebook PERSON\n"
     ]
    }
   ],
   "source": [
    "my_string = (u\"Mark Zuckerberg born May 14,1984 in New York is an American technology entreprenuer and philanthropist best known for co-funding and leading Facebook as it's chairman and CEO.\")\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:15:42.022100Z",
     "start_time": "2019-09-24T09:15:42.004162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:00 AM TIME\n",
      "90% PERCENT\n"
     ]
    }
   ],
   "source": [
    "my_string = u\"I usually wake up at 9:00 AM. 90% of my daytime goes in learning new things.\"\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:18:30.143611Z",
     "start_time": "2019-09-24T09:18:30.124699Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "my_string1 = u\"Imagine Dragons are the best band.\"\n",
    "my_string2 = u\"Imagine Dragons come and take over the city.\"\n",
    "\n",
    "doc1 = nlp(my_string1)\n",
    "doc2 = nlp(my_string2)\n",
    "\n",
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:19:31.060729Z",
     "start_time": "2019-09-24T09:19:31.057699Z"
    }
   },
   "outputs": [],
   "source": [
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- We were to extract the context of the above two strings in a live environment.\n",
    "- What to do? With help of Entity Extractor, one can easily figure out the statement and intelligently take the conversation further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are high-frequency words like a, an, the, to and also that we sometimes want to filter out a document before further processing.\n",
    "- These usually have little lexical content and do not hold much of a meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:25:33.567024Z",
     "start_time": "2019-09-24T09:25:33.561038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beforehand', 'did', 'his', 'should', 'cannot', \"'re\", 'our', 'therefore', '‘m', 'thereupon', 'which', 'same', 'former', 'per', 'part', 'whole', 'throughout', 'might', 'was', 'more', 'anywhere', 'quite', 'had', 'nowhere', 'always', 'thence', 'either', 'still', 'seems', 'rather', 'a', 'less', 'own', 'mostly', 'everything', 'been', 'now', 'thus', 'us', '’m', 'wherever', 'eight', 'whether', 'move', 'whereupon', \"'s\", '’ll', 'something', 'further', 'what', 'where', 'please', 'not', 'get', \"'d\", 'be', 'seeming', 'you', 'must', 'has', 'twelve', 'only', 'behind', 'afterwards', 'front', 'it', 'who', 'unless', 'onto', 'herein', 'fifteen', 'down', 'whoever', 'otherwise', 'two', 'their', 'no', 'last', 'ever', 'above', 'latter', 'her', 'ourselves', 'therein', 'can', 'yours', 'n’t', 'will', 'when', 'am', 'your', 'three', 'fifty', 'almost', 'being', 'whose', 'over', 'thereby', 'seem', 'them', 'against', 'under', 'myself', 'keep', 'an', 'to', 'by', 'thru', 'hereupon', 'most', 'they', 'such', 'yourselves', 'again', 'everyone', 'latterly', 'upon', 'although', 'does', 'besides', 'sometimes', 'also', 'often', 'during', '‘ve', 'into', 'first', 'empty', 'least', 'me', 'five', 'put', 're', 'elsewhere', 'say', 'until', 'whatever', 'about', 'enough', 'he', 'perhaps', \"'ve\", 'however', 'too', 'show', 'whom', 'somehow', 'name', 'that', 'for', 'becomes', 'have', 'without', 'himself', 'twenty', 'bottom', 'from', 'indeed', 'since', 'whereas', 'neither', 'as', 'thereafter', 'and', 'hence', \"n't\", 'mine', 'than', 'nothing', 'beyond', 'someone', 'already', 'four', 'made', 'others', 'namely', 'go', 'after', 'just', 'next', 'nine', 'hereby', 'noone', 'we', 'done', \"'m\", 'several', 'then', 'beside', 'yourself', 'because', 'may', 'him', '‘re', 'using', 'via', 'even', 'never', 'so', 'those', 'other', 'few', 'once', 'formerly', 'before', 'its', 'six', 'call', 'within', 'eleven', 'n‘t', '’re', '‘ll', 'amount', 'amongst', 'herself', 'ca', '’ve', 'some', 'hereafter', 'give', 'serious', 'though', 'various', 'with', 'are', 'out', 'meanwhile', 'below', 'each', 'many', 'any', '’s', 'yet', 'off', 'top', 'became', 'between', 'sometime', 'moreover', 'anyone', 'were', 'full', 'but', 'the', 'all', 'how', 'ours', 'take', 'could', 'whereby', 'whereafter', 'at', 'nevertheless', 'see', 'while', 'really', 'whenever', 'itself', 'among', 'this', 'why', 'whither', 'becoming', 'regarding', 'of', 'hundred', 'alone', \"'ll\", 'except', 'much', 'side', 'else', 'across', 'ten', 'would', 'or', 'she', 'on', 'these', 'another', 'back', 'towards', 'together', 'hers', 'sixty', 'wherein', 'very', 'through', 'doing', 'make', 'third', 'up', 'due', 'one', 'forty', 'there', 'toward', 'everywhere', 'seemed', 'every', 'nor', '‘d', 'well', 'in', 'my', 'anyway', 'whence', '’d', 'none', 'themselves', 'both', 'used', 'somewhere', '‘s', 'if', 'i', 'is', 'nobody', 'anything', 'anyhow', 'become', 'here', 'do', 'along', 'around'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:26:33.518430Z",
     "start_time": "2019-09-24T09:26:33.513480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check if a word is stop word or not\n",
    "nlp.vocab[u'is'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:44:57.172070Z",
     "start_time": "2019-09-24T09:44:57.166084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[u'hello'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:45:04.444342Z",
     "start_time": "2019-09-24T09:45:04.436397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[u'with'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Stop words are very important part of text cleanup. It helps removal of meaningless data before we try to do actual processing to make sense of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEPENDENCY PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:49:08.966675Z",
     "start_time": "2019-09-24T09:49:08.950718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[from, flight, Book]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"Book me a flight from Bangalore to Goa\")\n",
    "blr, goa = doc[5], doc[7]\n",
    "list(blr.ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:49:50.009920Z",
     "start_time": "2019-09-24T09:49:50.004934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[to, flight, Book]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(goa.ancestors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are ancestors in Dependency parsing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- They are the rightmost token of this token's syntactic descendants. Like in above example for the object blr the ancestors were from, flight, and Book.\n",
    "- You can always list the ancestors of a doc objects item using ancestors attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:52:41.739206Z",
     "start_time": "2019-09-24T09:52:41.735214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[flight, Book]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc[4].ancestors)                        # doc[4] == flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:53:18.283629Z",
     "start_time": "2019-09-24T09:53:18.278643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].is_ancestor(doc[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In real life scenario**<br><br>\n",
    "- If we try to think of a real world scenario that we might actually face while trying to build a chatbot, we may come across some sentence like: \"I want to book a cab to the hotel and a table at a restaurant\"\n",
    "- In this, its important to know what tasks are requested and where they are targeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T10:09:50.208221Z",
     "start_time": "2019-09-24T10:09:50.178302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booking of table belongs to restaurant\n",
      "Booking of table belongs to hotel\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Book a table at the restaurant and the taxi to the hotel\")\n",
    "tasks = doc[2], doc[8]           #(table, taxi)\n",
    "tasks_target = doc[5], doc[11]   #(restaurant, hotel)\n",
    "\n",
    "for task in tasks_target:\n",
    "    for tok in task.ancestors:\n",
    "        if tok in tasks:\n",
    "            print(\"Booking of {} belongs to {}\".format(tok,task))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are Children in Dependency Parsing?**<br>\n",
    "Children are immediate syntactic dependents of the token. We can see the children of a word by using children attribute just like we used ancestors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T10:11:43.568126Z",
     "start_time": "2019-09-24T10:11:43.563137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[restaurant]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc[3].children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interactive Visualization for Dependency Parsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T11:24:18.728633Z",
     "start_time": "2019-09-24T10:15:37.741508Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrshu\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"cd410e77d8c342ea873c5e7bf6f82691-0\" class=\"displacy\" width=\"2150\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Book</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">table</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">restaurant</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">taxi</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">hotel</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cd410e77d8c342ea873c5e7bf6f82691-0-0\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cd410e77d8c342ea873c5e7bf6f82691-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cd410e77d8c342ea873c5e7bf6f82691-0-1\" stroke-width=\"2px\" d=\"M70,352.0 C70,177.0 390.0,177.0 390.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cd410e77d8c342ea873c5e7bf6f82691-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,354.0 L398.0,342.0 382.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cd410e77d8c342ea873c5e7bf6f82691-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cd410e77d8c342ea873c5e7bf6f82691-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M560.0,354.0 L568.0,342.0 552.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cd410e77d8c342ea873c5e7bf6f82691-0-3\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cd410e77d8c342ea873c5e7bf6f82691-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cd410e77d8c342ea873c5e7bf6f82691-0-4\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cd410e77d8c342ea873c5e7bf6f82691-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cd410e77d8c342ea873c5e7bf6f82691-0-5\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cd410e77d8c342ea873c5e7bf6f82691-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cd410e77d8c342ea873c5e7bf6f82691-0-6\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cd410e77d8c342ea873c5e7bf6f82691-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cd410e77d8c342ea873c5e7bf6f82691-0-7\" stroke-width=\"2px\" d=\"M945,352.0 C945,89.5 1445.0,89.5 1445.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cd410e77d8c342ea873c5e7bf6f82691-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,354.0 L1453.0,342.0 1437.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cd410e77d8c342ea873c5e7bf6f82691-0-8\" stroke-width=\"2px\" d=\"M420,352.0 C420,2.0 1625.0,2.0 1625.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cd410e77d8c342ea873c5e7bf6f82691-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1625.0,354.0 L1633.0,342.0 1617.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cd410e77d8c342ea873c5e7bf6f82691-0-9\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,264.5 1960.0,264.5 1960.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cd410e77d8c342ea873c5e7bf6f82691-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,354.0 L1812,342.0 1828,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cd410e77d8c342ea873c5e7bf6f82691-0-10\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,177.0 1965.0,177.0 1965.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cd410e77d8c342ea873c5e7bf6f82691-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1965.0,354.0 L1973.0,342.0 1957.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "doc = nlp(u\"Book a table at the restaurant and the taxi to the hotel\")\n",
    "displacy.serve(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see one more example of dependency parsing where we assume a user is asking the following sentence:<br>\n",
    "*What are some places to visit in Berlin and stay in Lubeck?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T11:24:24.399243Z",
     "start_time": "2019-09-24T11:06:19.662Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(u\"What are some places to visit in Berlin and stay in Lubeck?\")\n",
    "places = [doc[7], doc[11]]                     #[Berlin, Lubeck]\n",
    "actions = [doc[5], doc[9]]                     #[visit, stay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T11:24:24.402236Z",
     "start_time": "2019-09-24T11:06:20.494Z"
    }
   },
   "outputs": [],
   "source": [
    "for place in places:\n",
    "    for tok in places.ancestors:\n",
    "        if tok in actions:\n",
    "            print(\"User is referring {} to {}\").format(place, tok)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the use of Dependency Parsing in Chatbots?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dependency parsing is one of the most important parts when handling chatbots from scratch. It becomes far more important when you want to figure out the meaning of a text input from your user to your chatbot.\n",
    "- There can be cases when we haven't trained our chatbots, but still we don't want to lose our customer or reply like a dumb machine.\n",
    "- Areas of help for dependency parsing:\n",
    "    - It helps in finding relationships between words of grammatically correct sentences.\n",
    "    - It can be used for sentence boundary detection.\n",
    "    - It is quite useful to find out if the user is talking about more than one context simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T11:25:33.286983Z",
     "start_time": "2019-09-24T11:25:33.269036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Boston Dynamics, thousands, robot dogs]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"Boston Dynamics is geairing up to produce thousands of robot dogs\")\n",
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T11:27:05.642484Z",
     "start_time": "2019-09-24T11:27:05.607632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning Learning nsubj cracks\n",
      "the code code dobj cracks\n",
      "messenger RNAs RNAs pobj of\n",
      "protein coding potential potential conj RNAs\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Deep Learning cracks the code of messenger RNAs and protein coding potential\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Text | Root.Text | Root.Dep_ | Root.head.text |\n",
    "|---------- | ------ | ------ | ------- |\n",
    "| deep learning | learning | nsubj | cracks |\n",
    "| the code | code | dobj | cracks |\n",
    "| messenger RNAs | RNAs | pobj | of |\n",
    "| protein-coding potential | potential | conj | RNAs |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column | Meaning |\n",
    "| ----- | ------ |\n",
    "| Text | Text of the original noun chunk |\n",
    "| Root text | Text of the original word that connects the noun chunk with remaining parse |\n",
    "| Root dep | Dependency relation that connects the root to its head |\n",
    "| Root head text | Text of the root token's head |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spaCy uses high-quality word vectors to find similarity between two words using GloVe algorithm(*Global Vectors for Word Representation*)\n",
    "- Unsupervised learning algorithm for obtaining vector representation for words.\n",
    "- Uses aggregated global word-word co-occurence statistics from a corpus to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T11:39:07.672023Z",
     "start_time": "2019-09-24T11:39:07.629142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How [-2.0411305  1.3408571 -0.4233122 -3.108604   0.836762 ]\n",
      "are [-1.3876543  0.6120126 -1.4433626  3.5829468 -0.7003968]\n",
      "you [-3.845181   -0.26770237  1.0238774  -1.9951785  -1.9080703 ]\n",
      "doing [-3.2571833  -3.1260393  -1.4001853  -0.36002517 -2.039889  ]\n",
      "today [ 2.7653427  -2.6375554   0.00717561 -0.3329475  -4.96414   ]\n",
      "? [ 0.7992259 -2.4167218  2.8654325  5.255692  -2.7328923]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"How are you doing today?\")\n",
    "for token in doc:\n",
    "    print(token.text, token.vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T11:41:40.454950Z",
     "start_time": "2019-09-24T11:41:40.391081Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrshu\\Anaconda3\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6450684542191895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrshu\\Anaconda3\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3945482191675344\n"
     ]
    }
   ],
   "source": [
    "hello_doc = nlp(u\"Hello\")\n",
    "hi_doc = nlp(u\"hi\")\n",
    "hella_doc = nlp(u\"hella\")\n",
    "print(hello_doc.similarity(hi_doc))\n",
    "print(hello_doc.similarity(hella_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "- If we see the word hello, it is more related and similar to the word hi, even though there is only difference of characters between the words hello and hella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T11:44:42.873479Z",
     "start_time": "2019-09-24T11:44:42.833532Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrshu\\Anaconda3\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7016174785258795"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GoT_str1 = nlp(u\"When will next season of Game of Thrones be releasing?\")\n",
    "GoT_str2 = nlp(u\"Game of Thrones next season release date?\")\n",
    "GoT_str1.similarity(GoT_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T11:51:41.002266Z",
     "start_time": "2019-09-24T11:51:40.868595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word car is 100% similar to word car\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrshu\\Anaconda3\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word car is 52% similar to word truck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrshu\\Anaconda3\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word car is 37% similar to word google\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrshu\\Anaconda3\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word truck is 52% similar to word car\n",
      "Word truck is 100% similar to word truck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrshu\\Anaconda3\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word truck is 38% similar to word google\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrshu\\Anaconda3\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word google is 37% similar to word car\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrshu\\Anaconda3\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word google is 38% similar to word truck\n",
      "Word google is 100% similar to word google\n"
     ]
    }
   ],
   "source": [
    "# Find similarity between words\n",
    "example_doc = nlp(u\"car truck google\")\n",
    "for t1 in example_doc:\n",
    "    for t2 in example_doc:\n",
    "        similarity_perc = int(t1.similarity(t2) * 100)\n",
    "        print(\"Word {} is {}% similar to word {}\".format(t1.text, similarity_perc, t2.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding similarity between words or sentences becomes quite important when we intend t build any application that is hugely dependent on the implementation of NLP.<br><br>\n",
    "When building chatbots, finding similarity can be very much handy for the following:\n",
    "- When building chatbots for recommendation\n",
    "- Removing duplicates\n",
    "- Building a spell-checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good to know things in NLP for Chatbots\n",
    "\n",
    "1. TOKENIZATION\n",
    "2. REGULAR EXPRESSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "- One of the simple yet basic concepts of NLP where we split text into meaningful segments.\n",
    "- spaCy first tokenizes the text(i.e., segments it into words and then punctuation and other things)\n",
    "- **One question**: Why can't we just use the built-in *split* method of Python Language and do the tokenization?\n",
    "    - Answer: The split method is just a raw method to split the sentence into tokens given a separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:02:02.379054Z",
     "start_time": "2019-09-24T12:02:02.352089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brexit\n",
      "is\n",
      "the\n",
      "impending\n",
      "withdrawal\n",
      "of\n",
      "the\n",
      "U.K.\n",
      "from\n",
      "the\n",
      "European\n",
      "Union\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Brexit is the impending withdrawal of the U.K. from the European Union.\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- See, U.K. comes as a single word after tokenization process, which makes sense, as U.K. is a country name and splitting it would be wrong.\n",
    "- Even after this if you are not happy with spaCy's tokenization, then you can use its add_special_case case method to add your own rule before relying completely on spaCy's tokenization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:18:09.754885Z",
     "start_time": "2019-09-24T12:18:09.751931Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence2 = \"Book me a metro from Airport Station to Hong Kong Station.\"\n",
    "sentence1 = \"Book me a cab to Hong Kong Airport from AsiaWorld-Expo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T12:18:10.444073Z",
     "start_time": "2019-09-24T12:18:10.435098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_to pattern matched correctly. Printing values\n",
      "\n",
      "From: Airport Station, To: Hong Kong Station.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from_to = re.compile('.* from (.*) to (.*)')\n",
    "to_from = re.compile('.* to (.*) from (.*)')\n",
    "\n",
    "from_to_match = from_to.match(sentence2)\n",
    "to_from_match = to_from.match(sentence2)\n",
    "\n",
    "if from_to_match and from_to_match.groups():\n",
    "    _from = from_to_match.groups()[0]\n",
    "    _to = from_to_match.groups()[1]\n",
    "    print(\"from_to pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from, _to))\n",
    "    \n",
    "elif to_from_match and to_from_match.groups():\n",
    "    _to = to_from_match.groups()[0]\n",
    "    _from = to_from_match.groups()[1]\n",
    "    print(\"to_from pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from, _to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
